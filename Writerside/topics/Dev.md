# å¼€å‘å®ç”¨

è®°å½•ä¸€äº›å¼€å‘å®ç”¨æ‰‹æ®µçš„å¿ƒå¾—

## ğŸ“ ç¬”è®°

### ç½‘ç«™æ¨è:

[çˆ¬è™«å·¥å…·åº“](https://spidertools.cn/) - åŒ…å«curl(f12ç½‘ç»œä¸­å³é”®å¤åˆ¶curl(bashæ ¼å¼))è½¬python requests, jsonä¸€é”®æ ¼å¼åŒ–å’Œå¯¹æ¯”ç­‰å¤šä¸ªåŠŸèƒ½

## ğŸ““ ä½¿ç”¨notebooké…åˆhusky-spider-utils

Jupyter Notebookæ˜¯çˆ¬è™«å¼€å‘çš„ç»ä½³å·¥å…·ï¼Œå¯ä»¥ä¸husky-spider-utilsæ— ç¼é›†æˆï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚

### å®‰è£…ä¸é…ç½®

```python
# å®‰è£…å¿…è¦çš„åŒ…
!pip install jupyter notebook
!pip install husky-spider-utils
```

### åŸºæœ¬ä½¿ç”¨

```python
from husky_spider_utils import SeleniumSession
import pandas as pd
import matplotlib.pyplot as plt

# åˆå§‹åŒ–ä¼šè¯
session = SeleniumSession("https://example.com")

# äº¤äº’å¼å¼€å‘ï¼Œå¯ä»¥ç«‹å³æŸ¥çœ‹ç»“æœ
response = session.get("https://example.com/data")
data = response.json()

# æ•°æ®åˆ†æä¸å¯è§†åŒ–
df = pd.DataFrame(data)
df.head()
```

### ä¼˜åŠ¿

1. **å³æ—¶åé¦ˆ** - æ¯ä¸ªä»£ç å—å¯ä»¥ç«‹å³æ‰§è¡Œå¹¶æŸ¥çœ‹ç»“æœ
2. **çŠ¶æ€ä¿æŒ** - æµè§ˆå™¨ä¼šè¯åœ¨æ•´ä¸ªnotebookç”Ÿå‘½å‘¨æœŸå†…ä¿æŒ
3. **å¯è§†åŒ–** - ç›´æ¥åœ¨notebookä¸­æŸ¥çœ‹æ•°æ®ã€å›¾è¡¨å’ŒHTMLå†…å®¹
4. **æ–‡æ¡£åŒ–** - ä»£ç ã€æ³¨é‡Šå’Œç»“æœä¸€ä½“åŒ–ï¼Œä¾¿äºåˆ†äº«å’Œå¤ç”¨

### å®ç”¨æŠ€å·§

```python
# åœ¨notebookä¸­æ˜¾ç¤ºç½‘é¡µæˆªå›¾
from IPython.display import Image, display
import base64

def show_screenshot(session):
    screenshot = session.driver.get_screenshot_as_base64()
    display(Image(data=base64.b64decode(screenshot)))

# ä½¿ç”¨ç¤ºä¾‹
session.get("https://example.com")
show_screenshot(session)
```

## ğŸ› ï¸ åˆç†ä½¿ç”¨devtools(F12å¼€å‘è€…å·¥å…·)

æµè§ˆå™¨å¼€å‘è€…å·¥å…·æ˜¯çˆ¬è™«å¼€å‘çš„å¼ºå¤§åŠ©æ‰‹ï¼ŒæŒæ¡å®ƒå¯ä»¥å¤§å¤§æé«˜æ•ˆç‡ã€‚

### å…ƒç´ é€‰æ‹©å™¨(Elements)

1. **å¿«é€Ÿå®šä½å…ƒç´ **
   - ä½¿ç”¨å…ƒç´ é€‰æ‹©å·¥å…·(ğŸ”)ç‚¹å‡»é¡µé¢å…ƒç´ 
   - å³é”®å…ƒç´  â†’ æ£€æŸ¥
   - å¤åˆ¶é€‰æ‹©å™¨: å³é”®HTMLå…ƒç´  â†’ Copy â†’ Copy selector

2. **éªŒè¯CSSé€‰æ‹©å™¨**
   - åœ¨Consoleé¢æ¿ä¸­ä½¿ç”¨`document.querySelector()`æµ‹è¯•é€‰æ‹©å™¨
   ```javascript
   document.querySelector('.my-class') // æµ‹è¯•å•ä¸ªå…ƒç´ 
   document.querySelectorAll('.item') // æµ‹è¯•å¤šä¸ªå…ƒç´ 
   ```

### ç½‘ç»œç›‘æ§(Network)

1. **åˆ†æè¯·æ±‚**
   - ç­›é€‰XHRè¯·æ±‚æ‰¾åˆ°APIç«¯ç‚¹
   - æŸ¥çœ‹è¯·æ±‚å¤´ã€å‚æ•°å’Œå“åº”
   - å³é”®è¯·æ±‚ â†’ Copy â†’ Copy as cURLï¼Œç„¶åä½¿ç”¨`curl_cffi`åœ¨Pythonä¸­é‡ç°

2. **æ¨¡æ‹Ÿè¯·æ±‚**
   ```python
   # ä»å¼€å‘è€…å·¥å…·å¤åˆ¶çš„è¯·æ±‚å¤´
   headers = {
       'User-Agent': '...',
       'Referer': '...',
       'X-Requested-With': 'XMLHttpRequest'
   }
   
   # åœ¨husky-spider-utilsä¸­ä½¿ç”¨
   session = SeleniumSession()
   response = session.get("https://api.example.com/data", headers=headers)
   ```

### JavaScriptè°ƒè¯•(Console)

1. **æå–éšè—æ•°æ®**
   - è®¸å¤šç½‘ç«™åœ¨å…¨å±€å˜é‡ä¸­å­˜å‚¨æ•°æ®
   ```javascript
   // åœ¨Consoleä¸­æ‰§è¡Œ
   window.__INITIAL_DATA__ // æŸ¥æ‰¾é¢„åŠ è½½çš„æ•°æ®
   ```

   ```python
   # åœ¨Pythonä¸­æå–
   data = session.execute_script("return window.__INITIAL_DATA__;")
   ```


## ğŸ“Š è°ƒè¯•ä¸åˆ†ææŠ€å·§

### è®°å½•å…³é”®ä¿¡æ¯

```python
from husky_spider_utils import SeleniumSession
from loguru import logger

# åˆ›å»ºè‡ªå®šä¹‰çˆ¬è™«
class DebugSpider(SeleniumSession):
    def __init__(self):
        super().__init__("https://example.com")
        
    def parse_page(self, url):
        self.get(url)
        # ä¿å­˜é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•
        self.driver.save_screenshot(f"debug_{url.split('/')[-1]}.png")
        # ä¿å­˜é¡µé¢æºç 
        with open(f"debug_{url.split('/')[-1]}.html", "w", encoding="utf-8") as f:
            f.write(self.get_page_source())
```

### æ€§èƒ½ä¼˜åŒ–

1. **ä½¿ç”¨æ— å¤´æ¨¡å¼æé«˜é€Ÿåº¦**
   ```python
   from selenium.webdriver.firefox.options import Options

   options = Options()
   options.headless = True
   
   session = SeleniumSession(option=options)
   ```


## ğŸ”„ æŒç»­é›†æˆä¸è‡ªåŠ¨åŒ–

### å®šæ—¶ä»»åŠ¡

```python
# ä½¿ç”¨scheduleåº“åˆ›å»ºå®šæ—¶ä»»åŠ¡
import schedule
import time
from my_spider import DataSpider

def job():
    spider = DataSpider()
    try:
        spider.run()
    finally:
        spider.quit()

# æ¯å¤©æ—©ä¸Š8ç‚¹è¿è¡Œ
schedule.every().day.at("08:00").do(job)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### æ•°æ®æŒä¹…åŒ–

```python
import pandas as pd
from husky_spider_utils import SeleniumSession

session = SeleniumSession()
results = []

# çˆ¬å–æ•°æ®
urls = ["https://example.com/page1", "https://example.com/page2"]
for url in urls:
    session.get(url)
    selector = session.get_element_selector(".item")
    items = selector.css(".item-title::text").getall()
    prices = selector.css(".item-price::text").getall()
    
    for title, price in zip(items, prices):
        results.append({"title": title.strip(), "price": price.strip()})

# ä¿å­˜ä¸ºCSV
df = pd.DataFrame(results)
df.to_csv("results.csv", index=False, encoding="utf-8")

# æˆ–ä¿å­˜ä¸ºæ•°æ®åº“
import sqlite3
conn = sqlite3.connect("results.db")
df.to_sql("items", conn, if_exists="append", index=False)
```

---

é€šè¿‡ç»“åˆJupyter Notebookã€æµè§ˆå™¨å¼€å‘è€…å·¥å…·å’Œhusky-spider-utilsçš„å¼ºå¤§åŠŸèƒ½ï¼Œä½ å¯ä»¥æ„å»ºæ›´é«˜æ•ˆã€æ›´å¯é çš„çˆ¬è™«åº”ç”¨ã€‚æŒç»­å­¦ä¹ å’Œå®è·µè¿™äº›æŠ€å·§ï¼Œå°†å¤§å¤§æå‡ä½ çš„çˆ¬è™«å¼€å‘æ•ˆç‡ã€‚
